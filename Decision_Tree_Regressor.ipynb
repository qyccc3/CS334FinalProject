{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statistics import mean\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import graphviz\n",
        "from sklearn import tree"
      ],
      "metadata": {
        "id": "ySgDxa5xy2rR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement 5-fold nested CV to find optimal parameter configuration for the model. Report: (1) tree visualization (2) the set of best parameters on each split (3) testing mse on each outer split (3) important splits / features (4) algorithm runtime"
      ],
      "metadata": {
        "id": "oDJZsZt5vpi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.get_dummies(pd.read_csv('ManualPreprocessedAmesHousing.csv'))\n",
        "\n",
        "X = data.drop('SalePrice', axis=1).to_numpy()\n",
        "y = data['SalePrice'].to_numpy()\n",
        "\n",
        "max_depth = range(5, 30)\n",
        "min_samples_leaf = range(2, 10)\n",
        "\n",
        "best = {}\n",
        "\n",
        "outer_kfold = KFold(n_splits=5)\n",
        "inner_kfold = KFold(n_splits=4)\n",
        "\n",
        "for train_index, test_index in outer_kfold.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    best_max_depth = 0\n",
        "    best_min_samples_leaf = 0\n",
        "    best_mse = float('inf')\n",
        "    for depth in max_depth:\n",
        "        for samples in min_samples_leaf:\n",
        "            mse = []\n",
        "            for train_inner_index, _ in inner_kfold.split(X_train):\n",
        "                test_inner_index = np.array(list(set(train_index) - set(train_inner_index)))\n",
        "                X_train_inner, X_test_inner = X[train_inner_index], X[test_inner_index]\n",
        "                y_train_inner, y_test_inner = y[train_inner_index], y[test_inner_index]\n",
        "                dtr = DecisionTreeRegressor(max_depth=depth, min_samples_leaf=samples)\n",
        "                dtr.fit(X_train_inner, y_train_inner)\n",
        "                mse.append(mean_squared_error(y_test_inner, dtr.predict(X_test_inner)))\n",
        "            avg_mse = mean(mse)\n",
        "\n",
        "            if avg_mse < best_mse:\n",
        "                best_mse = avg_mse\n",
        "                best_max_depth = depth\n",
        "                best_min_samples_leaf = samples\n",
        "    dtr = DecisionTreeRegressor(max_depth=best_max_depth, min_samples_leaf=best_min_samples_leaf)\n",
        "    dtr.fit(X_train, y_train)\n",
        "    score = mean_squared_error(y_test, dtr.predict(X_test))\n",
        "    print('for this split, best tuning average mse is', best_mse, 'best max depth is', best_max_depth, 'best min samples leaf is', best_min_samples_leaf, 'the testing mse is', score)\n",
        "    tree_viz = tree.export_graphviz(dtr, out_file=None, max_depth=3,\n",
        "                                feature_names = data.drop('SalePrice', axis=1).columns,  \n",
        "                                class_names = str(data['SalePrice']),\n",
        "                                rounded = True,\n",
        "                                special_characters = True,\n",
        "                                filled = True)\n",
        "    graph = graphviz.Source(tree_viz, format=\"png\") \n",
        "    graph.render(\"decision tree\"+\" \"+str(best_max_depth)+\" \"+str(best_min_samples_leaf)+\".png\")\n"
      ],
      "metadata": {
        "id": "19plaMTsVXT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aafdc10-36dc-4a7a-f056-e7041caca061"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for this split, best tuning average mse is 1161.3889511218356 best max depth is 10 best min samples leaf is 6 the testing mse is 1184.072415368033\n",
            "for this split, best tuning average mse is 1240.1566045550055 best max depth is 10 best min samples leaf is 3 the testing mse is 816.6995644493976\n",
            "for this split, best tuning average mse is 1205.4956022292404 best max depth is 17 best min samples leaf is 9 the testing mse is 1368.8059187141284\n",
            "for this split, best tuning average mse is 1101.8455810768055 best max depth is 8 best min samples leaf is 3 the testing mse is 2051.211779566418\n",
            "for this split, best tuning average mse is 1369.1129161089543 best max depth is 22 best min samples leaf is 6 the testing mse is 1058.4104642453954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch (Hide for now)"
      ],
      "metadata": {
        "id": "LSo7QRy63Qsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch_data = torch.tensor(data.drop('SalePrice', axis=1).values).float().to(device)\n",
        "# print(torch_data.shape)\n",
        "# torch_label = torch.tensor(data['SalePrice'].values).float().to(device)"
      ],
      "metadata": {
        "id": "y-UPoP58ewSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class linearRegression(torch.nn.Module):\n",
        "#     def __init__(self, inputDim, hiddenDim):\n",
        "#         super(linearRegression, self).__init__()\n",
        "#         layers = []\n",
        "#         in_dim = inputDim\n",
        "#         for hidden_dim in hiddenDim[:-1]:\n",
        "#             layers.append(torch.nn.Linear(in_dim, hidden_dim))\n",
        "#             layers.append(torch.nn.BatchNorm1d(hidden_dim))\n",
        "#             layers.append(torch.nn.ReLU())\n",
        "#             layers.append(torch.nn.Dropout(p=0.5))\n",
        "#             in_dim = hidden_dim\n",
        "#         layers.append(torch.nn.Linear(in_dim, hiddenDim[-1]))\n",
        "#         layers.append(torch.nn.BatchNorm1d(hiddenDim[-1]))\n",
        "#         layers.append(torch.nn.ReLU())\n",
        "#         layers.append(torch.nn.Dropout(p=0.5))\n",
        "#         self.net = torch.nn.Sequential(*layers)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.net(x)\n",
        "#         return out"
      ],
      "metadata": {
        "id": "G1E0BFmiByn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hidden_dim = [176, 1]\n",
        "# model = linearRegression(torch_data.shape[1], hidden_dim).to(device)\n",
        "# # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR()\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "# criterion = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "fQcQMl3dZlTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def evaluate(xTrain, xTest, yTrain, yTest, batch_size):\n",
        "#   stats = {}\n",
        "#   train_mse = []\n",
        "#   test_mse = []\n",
        "\n",
        "#   for epoch in range(750):\n",
        "#     print(epoch, end=' ')\n",
        "#     for i in range(0, xTrain.shape[0], batch_size):\n",
        "#       optimizer.zero_grad()\n",
        "#       y_pred = model(xTrain[i:(i+batch_size), :])\n",
        "#       loss = torch.sqrt(criterion(y_pred, yTrain[i:(i+batch_size)].view(-1,1)))\n",
        "#       loss.backward()\n",
        "#       optimizer.step()\n",
        "#     train_mse.append(loss.item())\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#       y_hat = model(xTest)\n",
        "#       print(y_hat[0], yTest[0])\n",
        "#       loss = criterion(y_hat, yTest.view(-1,1))\n",
        "#       test_mse.append(loss.item())\n",
        "\n",
        "#   stats['train'] = train_mse\n",
        "#   stats['test'] = test_mse\n",
        "\n",
        "#   fig = plt.figure()\n",
        "#   for k, v in stats.items():\n",
        "#       plt.plot(range(1, len(v) + 1), v, label=k)\n",
        "\n",
        "#   plt.legend()\n",
        "#   plt.xlabel('Epochs')\n",
        "#   plt.ylabel('MSE')\n",
        "#   plt.title('Train Test MSE')\n",
        "#   fig.show()\n",
        "\n",
        "#   return stats"
      ],
      "metadata": {
        "id": "fgfE2oQZB16M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kf = KFold(n_splits=3)\n",
        "# for train_index, test_index in kf.split(torch_data):\n",
        "#   X_train, X_test = torch_data[train_index], torch_data[test_index]\n",
        "#   y_train, y_test = torch_label[train_index], torch_label[test_index]\n",
        "\n",
        "#   _ = evaluate(X_train, X_test, y_train, y_test, batch_size=128)\n",
        "#   # Stop here for now\n",
        "#   x == 5"
      ],
      "metadata": {
        "id": "XEuk7CVbch69"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}